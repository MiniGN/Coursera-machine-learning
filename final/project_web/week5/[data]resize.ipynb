{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob,os,pickle\n",
    "from tqdm import tqdm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Получаем ИД пользователя из имени файла\n",
    "def get_user_id(filename):\n",
    "    start_pos=(filename.find('\\\\user'))+5\n",
    "    end_pos=(filename.find('.csv'))\n",
    "    return int(filename[start_pos:end_pos])\n",
    "\n",
    "# Преобразование логов пользователя в таблицу сессий\n",
    "def user_log_to_session(user_log_df,user_id,session_length,window_size):\n",
    "    res=pd.DataFrame()\n",
    "    # Добавляем session_length новых колонок виде site_i, где новая значение колонки берется как колонка site со сдвигом на i строк\n",
    "    # получаем таблицу вида:\n",
    "    # site1 time1 site2 time2 site3 time3\n",
    "    # site2 time2 site3 time3 site4 time4\n",
    "    for col in range(1,session_length+1):\n",
    "        shift_index=col-1\n",
    "        site_col_name='site'+str(col)\n",
    "        res[site_col_name]=user_log_df.site[shift_index:]\n",
    "        res[site_col_name]=res[site_col_name].shift(-shift_index)\n",
    "        timestamp_col_name='time'+str(col)\n",
    "        res[timestamp_col_name]=user_log_df.timestamp[shift_index:]\n",
    "        res[timestamp_col_name]=res[timestamp_col_name].shift(-shift_index)\n",
    "    # Удаляем лишние строки, оставляя только строки которые соответствуюи ширине окна window_size\n",
    "    res['user_id']=user_id\n",
    "    ind_tosave=[x for x in range(0,len(user_log_df),window_size)]\n",
    "    return res.iloc[ind_tosave,:]\n",
    "\n",
    "# Фуникция читает логи пользователей, склеивает их в единый DataFrame и заменяет названия сайтов на идентификаторы по словарю\n",
    "def read_and_replace(path_to_csv_files):\n",
    "    print('1. Читаем исходные логи пользователей')\n",
    "    all_files = glob.glob(os.path.join(path_to_csv_files, \"*.csv\"))\n",
    "    user_log=[]\n",
    "    # Последовательно читаем все файлы логов пользоваителей в каталоге и склеиваем их в один DataFrame \n",
    "    for filename in all_files:\n",
    "        df_user=pd.read_csv(filename)\n",
    "        df_user['user_id']=get_user_id(filename)\n",
    "        user_log.append(df_user)\n",
    "    all_user_log = pd.concat(user_log, ignore_index=True)\n",
    "    # Заменяем название сайта на ИД по словарю\n",
    "    print('2. Заменяем название сайта на ИД по словарю')\n",
    "    with open('site_dic.pkl', 'rb') as file_pkl:\n",
    "        site_dic=pickle.load(file_pkl)\n",
    "    all_user_log.site=all_user_log.site.map(site_dic) \n",
    "    return all_user_log\n",
    "    \n",
    "# Функция преобразовывает логи пользователя по посещению сайтов в таблицу сессий\n",
    "def prepare_train_set(all_user_log, session_length,window_size):\n",
    "    print('3. Формируем сессии')\n",
    "    sessions=pd.DataFrame()\n",
    "    for user_id in tqdm(all_user_log.user_id.value_counts().index):\n",
    "        # Получаем логи одного пользователя\n",
    "        user_df=all_user_log[all_user_log.user_id==user_id]\n",
    "        # Формируем для одного пользователя таблицу сессий\n",
    "        user_session=user_log_to_session(user_df,user_id,session_length,window_size)\n",
    "        # Добавляем таблицу сессий пользователя в общую таблицу\n",
    "        sessions=pd.concat([sessions,user_session])\n",
    "    return sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1. Читаем исходные логи пользователей\n",
      "2. Заменяем название сайта на ИД по словарю\n",
      "window_size= 3\n",
      "3. Формируем сессии\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "  0%|                                                                                          | 0/400 [00:00<?, ?it/s]\n",
      "  0%|▏                                                                                 | 1/400 [00:00<02:00,  3.31it/s]\n",
      "  0%|▍                                                                                 | 2/400 [00:00<01:51,  3.57it/s]\n",
      "  1%|▌                                                                                 | 3/400 [00:00<01:43,  3.85it/s]\n",
      "  1%|▊                                                                                 | 4/400 [00:01<01:46,  3.71it/s]\n",
      "  1%|█                                                                                 | 5/400 [00:01<01:42,  3.85it/s]\n",
      "  2%|█▏                                                                                | 6/400 [00:01<01:40,  3.93it/s]\n",
      "  2%|█▍                                                                                | 7/400 [00:01<01:39,  3.95it/s]\n",
      "  2%|█▋                                                                                | 8/400 [00:02<01:39,  3.93it/s]\n",
      "  2%|█▊                                                                                | 9/400 [00:02<01:42,  3.83it/s]\n",
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [06:35<00:00,  1.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size= 5\n",
      "3. Формируем сессии\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [05:14<00:00,  1.13s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size= 7\n",
      "3. Формируем сессии\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [03:51<00:00,  1.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "window_size= 10\n",
      "3. Формируем сессии\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 400/400 [03:01<00:00,  1.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wall time: 18min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "folder='train/'\n",
    "# Фуникция формирует таблицу сессий с заданным ширином окна и сохраняет данные в файл\n",
    "def resize_train(path_to_csv_files, session_length,window_size):\n",
    "    sessions=prepare_train_set(all_user_log, session_length,window_size)\n",
    "    with open('train_'+str(window_size)+'_session.pkl', 'wb') as pkl_file:\n",
    "        pickle.dump(sessions, pkl_file)\n",
    "all_user_log=read_and_replace(folder)\n",
    "for window_size in [3,5,7,10]:\n",
    "    print ('window_size=',window_size)\n",
    "    resize_train(all_user_log,10,window_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Прежняя реализация"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:00<00:00, 11.41it/s]\n"
     ]
    }
   ],
   "source": [
    "sessions=pd.DataFrame()\n",
    "session_length=10\n",
    "window_size=3\n",
    "for user_id in tqdm(a.user_id.value_counts().index):\n",
    "    user_df=a[a.user_id==user_id]\n",
    "    user_session=user_log_to_session(user_df,user_id,session_length,window_size)\n",
    "    sessions=pd.concat([sessions,user_session])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "window_size=3\n",
    "session_length=10\n",
    "max_sesssion_len=200\n",
    "# Обрабатываем лог юзера и возвращем индексы записей лога, которые должны войти в одну сессию \n",
    "def process_user(user_df,window_size,session_length,max_sesssion_len):\n",
    "    session_coords=[]\n",
    "    start_index=user_df.index.min()\n",
    "    max_index=user_df.index.max()\n",
    "    while start_index<max_index:\n",
    "        for pad in range(session_length):\n",
    "            end_index=start_index+session_length-pad-1\n",
    "            if end_index>max_index:\n",
    "                end_index=max_index\n",
    "            time1=datetime.strptime(user_df.loc[start_index,'timestamp'], '%Y-%m-%d %H:%M:%S')\n",
    "            time2=datetime.strptime(user_df.loc[end_index,'timestamp'], '%Y-%m-%d %H:%M:%S')\n",
    "            session_len=(time2-time1).total_seconds()\n",
    "            if session_len<=max_sesssion_len:\n",
    "                break\n",
    "        session_coords.append([start_index,end_index])\n",
    "        start_index+=window_size\n",
    "    return (session_coords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████████| 2/2 [00:06<00:00,  3.19s/it]\n"
     ]
    }
   ],
   "source": [
    "session_coords=[]\n",
    "for user_id in tqdm(a.user_id.value_counts().index):\n",
    "    user_df=a[a.user_id==user_id]\n",
    "    session_coords=process_user(user_df,window_size,session_length,max_sesssion_len)\n",
    "    sessiond_df=log_to_session(session_coords,user_df,user_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def log_to_session(session_coords,user_df,user_id):\n",
    "    sessions_num=len(session_coords)\n",
    "    sessions=np.array([])\n",
    "    for coord in session_coords:\n",
    "        session_log=user_df.iloc[coord[0]:coord[1]+1,:][['timestamp','site_id']].as_matrix()\n",
    "        if len(session_log) < windows_size:\n",
    "            ar_pad=np.empty(2*(windows_size-len(session_log)))\n",
    "            ar_pad.fill(np.nan)\n",
    "            session_log=np.append(session_log, ar_pad)\n",
    "        sessions=np.append(sessions, session_log)\n",
    "        sessions=np.append(sessions, user_id)\n",
    "    col_name=np.array([['time'+str(x),'site'+str(x)] for x in range(1,windows_size+1)]).reshape([1,2*windows_size])[0]\n",
    "    col_name=np.append(col_name, ['user_id'])\n",
    "    sessions=pd.DataFrame(sessions.reshape([sessions_num,1+2*windows_size]),columns=col_name)\n",
    "    return sessions\n",
    "    \n",
    "t=log_to_session(session_coords,a,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Взвращаем ИД сайта по его названию\n",
    "def get_site_id(site,site_dict):\n",
    "    print ('site:',site)\n",
    "    return site_dict[site][0]\n",
    "# Заменяем названия сайтов на ИД, формируем словарь частот сайтов\n",
    "def get_site_dict(all_user_log):\n",
    "    # Расчитываем частоту freq для каждого сайта \n",
    "    all_user_log['freq'] = all_user_log.groupby('site')['site'].transform('count')\n",
    "    # Формируем словарь\n",
    "    site_dict=all_user_log[['site','freq']].drop_duplicates().sort_values(by='freq',ascending=False).reset_index(drop=True)\n",
    "    site_dict['site_id']=site_dict.index+1\n",
    "    site_dict.set_index(site_dict.site,inplace=True)\n",
    "    site_dict['site_freq']=site_dict.apply(lambda row: (row['site_id'],row['freq']), axis=1)\n",
    "    all_user_log=all_user_log.join(site_dict[['site','site_id']],on=['site'],rsuffix='r')\n",
    "    site_dict=site_dict.site_freq.to_dict()\n",
    "    return all_user_log[['timestamp','site_id','user_id']],site_dict\n",
    "\n",
    "\n",
    "# Функция возвращает подготовленные данные и словарь сайтов\n",
    "def prepare_train_set2(path_to_csv_files, session_length,window_size):\n",
    "    # Последовательно читаем все файлы сессий в каталоге и склеиваем их в один DataFrame = sessions_df\n",
    "    all_files = glob.glob(os.path.join(path_to_csv_files, \"*.csv\"))\n",
    "    print('1. Читаем исходные данные')\n",
    "    user_log=[]\n",
    "    for filename in all_files:\n",
    "        df_user=pd.read_csv(filename)\n",
    "        df_user['user_id']=get_user_id(filename)\n",
    "        user_log.append(df_user)\n",
    "    all_user_log = pd.concat(user_log, ignore_index=True)\n",
    "    print('2. Формируем словарь')\n",
    "    # Формируем словарь сайтов\n",
    "    all_user_log,site_dict=get_site_dict(all_user_log)\n",
    "    print('3. Формируем сессии')\n",
    "    sessions=pd.DataFrame()\n",
    "    for user_id in tqdm(all_user_log.user_id.value_counts().index):\n",
    "        user_df=all_user_log[all_user_log.user_id==user_id]\n",
    "        user_session=user_log_to_session(user_df,user_id,session_length,window_size)\n",
    "        sessions=pd.concat([sessions,user_session])\n",
    "    return sessions,site_dict"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
